Лабораторная работа 4.  
====
# Немного информации о цели лабараторной работы
Исследовать влияние различных техник аугментации данных на процесс обучения нейронной сети на примере решения задачи классификации Oregon Wildlife с использованием техники обучения Transfer Learning. В данной работе использовалась нейронная сеть EfficientNet-B0, предобученная  на базе изображений ImageNet с использованием политики изменения темпа обучения - Экспоненциальное затухание (Exponential Decay) с параметрами initial_lrate = 0.01 и k = 0.25. 

Аугментация данных — это важный этап обучения моделей машинного обучения. Под аугментацией данных понимается увеличение выборки данных для обучения через модификацию существующих данных.

В данной лабораторной работе использовались техники аугментации данных:
 -Манипуляции с яркостью и контрастом
 -Поворот изображения на случайный угол
 -Использование случайной части изображения
 -Добавление случайного шума
 
 * Для упрощения анализа результатов, будем сравнивать их с резульататми полученными при обучении нейронной сети EfficientNet-B0 без аугментации данных.
 
  ### Графики обучения для предобученной нейронной сети EfficientNet-B0 с политикой изменения темпа обучения - экспоненциальное затухание
 

 ***График метрики точности:*** 
<img src="./graph/epoch_categorical_accuracy_main.svg">

 ***График функции потерь::*** 
 
<img src="./graph/epoch_loss_main.svg">

### Анализ результатов:
- скорость схождения алгоритма = 27 эпох
- точности в конце обучения = 0.8795
- значение потерь в конце обучения = 0.326
 

# С использованием техники обучения Transfer Learning и оптимальной политики изменения темпа обучения обучить нейронную сеть EfficientNet-B0 (предварительно обученную на базе изображений imagenet) для решения задачи классификации изображений Oregon WildLife с использованием следующих техник аугментации данных:
 
### Манипуляции с яркостью и контрастом

* Функции изменения контраста, где image - входное изображение, 0.5  - множитель для регулировки контраста (contrast_factor).
```
def contrast(image, label):  
  return tf.image.adjust_contrast(image, 0.5), label
```

* Функции изменения яркости, где image - входное изображение, delta = 0.1  - число добавляемое к значениям пикселей.
```
def brightness(image, label):
  return tf.image.adjust_brightness(image, delta=0.1),label
```

* Вызов функции производился в TFRecordDataset:
```
return tf.data.TFRecordDataset(filenames)\
    .....
    .map(contrast)\
    .map(brightness)\
    ....

```

* Использовались следующии параметры:
```
  contrast_factor = 0.5, delta = 0.5;
  contrast_factor = 2, delta = 0.1;
  contrast_factor = 2, delta = 0.25;
  contrast_factor = 2, delta = 0.5;
  contrast_factor = 4, delta = 0.1;
```

 ### Графики обучения для предобученной нейронной сети EfficientNet-B0 с манипуляции с яркостью и контрастом:
  
 ***График метрики точности:*** 
<img src="./graph/epoch_categorical_accuracy_b_c.svg">

***Пояснение:*** 
 
<img src="./graph/b_c_cat.jpg">

 ***График функции потерь:*** 
 
<img src="./graph/epoch_loss_b_c.svg">

 ***Пояснение:*** 
 
<img src="./graph/b_c_loss.jpg">


### Анализ результатов:
Смотря на график метрики точности видно, что лучшее значения, достигаются при использовании темпов обучения 0.001, равное 87.92%, и 0.1, равное 87.32% . Смотря на график функции потерь видно, что минимальные потери были при темпах обучения 0.001 и 0.0001. Исходя из данных результатов можно сказать, что темп обучения 0.001 является оптимальным.


# Реализовать и применить в обучении следующие политики изменения темпаобучения, а также определить оптимальные параметры для каждой политики: Пошаговое затухание (Step Decay) и Экспоненциальное затухание (Exponential Decay)

* **Описание структуры** 


* Функция пошагового затухания, где initial_lrate = 0.0001 - начальный темп обучения,  drop = 0.25 - изменения темпа обучения, в моем случае в 4 раза, epochs_drop = 5.0 - каждый 5 эпох будет изменять темп обучения.
```
def step_decay(epoch,layer):
  initial_lrate = 0.01
  drop = 0.25
  epochs_drop = 5.0
  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))
  return lrate
```


* Функция экспоненциального затухания, где initial_lrate = 0.001 - начальный темп обучения,  k = 0.1 - коэффициент 
```
def exp_decay(epoch,lr):
  initial_lrate = 0.01
  k = 0.5
  lrate = initial_lrate * math.exp(-k*epoch)
  return lrate
```


 ### Графики обучения для предобученной нейронной сети EfficientNet-B0 с политикой изменения темпа обучения - пошаговое затухание
 

 ***График метрики точности:*** 
<img src="./graph/epoch_categorical_accuracy_step.svg">

 ***График функции потерь:*** 
 
<img src="./graph/epoch_loss_step.svg">

 ***Пояснение:*** 
 
<img src="./graph/com_step.jpg">

### Анализ результатов:
Смотря на график метрики точности и функции потерь видно, что лучшее значения, достигаются при использовании параметров initial_lrate = 0.001, drop = 0.5, epochs_drop = 5.0. Получилось значение точности равное 88.52%, что на 0.60% лучше, чем при фиксированном темпе обучение. Также при данных параметрах достигается наилучшее значение функции потерь равное 0.2313.

 ### Графики обучения для предобученной нейронной сети EfficientNet-B0 с политикой изменения темпа обучения - экспоненциальное затухание
 

 ***График метрики точности:*** 
<img src="./graph/epoch_categorical_accuracy_exp.svg">

 ***График функции потерь::*** 
 
<img src="./graph/epoch_loss_exp.svg">

 ***Пояснение:*** 
 
<img src="./graph/com_exp.jpg">

### Анализ результатов:
Смотря на график метрики точности и функции потерь видно, что лучшее значения, достигаются при использовании параметров initial_lrate = 0.01 и k = 0.25. Получилось значение точности равное 89.07%, что на 1.15% лучше, чем при фиксированном темпе обучение. 


 ### Итоговое сравнение оптимальных результатов
 

***График метрики точности:*** 
<img src="./graph/epoch_categorical_accuracy_opt.svg">

 ***График функции потерь::*** 
 
<img src="./graph/epoch_loss_opt.svg">

 ***Пояснение:*** 
 
<img src="./graph/com_opt.jpg">

### Анализ результатов:
* Из графика метрики точности, можно сказать что лучший результат равный 89.07% достигается при использовании политики изменения темпа обучения - экспоненциальное затухание, что на 1.15% лучше, чем при фиксированном темпе обучение, и на 0.55% лучше, чем при пошаговом затухании.

* Из графика функции потерь, можно сказать что лучший результат 0.2313 равный достигается при использовании политики изменения темпа обучения - пошаговом затухании, что на 0.1748 меньше чем при фиксированном темпе обучения, и на 0.0631 меньше чем при экспоненциальном затухании.

* Оптимальный результат получен при использовании политики изменения темпа обучения - экспоненциальное затухание.
